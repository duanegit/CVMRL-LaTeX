\chapter{Introduction}\label{ch:Introduction}

In recent years, a number of researchers have been working to adapt
technology developed for robotic control to use in the
creation of high-technology assistive devices for the visually
impaired.  These types of devices have been proven to help visually
impaired people live with a greater degree of confidence and
independence.  However, most prior work has focused primarily on a single
problem from mobile robotics, namely navigation in an
unknown environment.  The result of which is to either guide the user along an unobstructed path, or communicate the location of obstacles, and let the user determine their own path~\cite{Bib:Ram, Bib:Meers}. The latter scheme accounts for the notion that in the world of mobile robotics, artificial intelligence is the fundamental limiting factor~\cite{Bib:Parker}.  With current technology, an autonomous robot's ability to sense and process information about its environment far surpasses its decision-making capability.

In our previous work \cite{JACQUES05, JACQUES07} we presented a prototype assistive device aimed at providing an initial solution to the largely unaddressed problem of guiding a visually impaired person's hand to a target to complete a goal-oriented reaching task.
The unit was a wearable assistive device which performed object tracking and visual servoing for a visually impaired user.  The system captures images from a glove mounted camera, detects a given object of interest and directs the user's hand toward that target via a set of motion cues through a vibrotactile interface.  That initial work is presented here, along with proposed further developments in terms of a model for the system using a supervisory hybrid control scheme.  A significant issue that is addressed is the difficulty with measuring system performance given that from one user to another, and even across instances of usage by a given user, expected performance can vary greatly.  So we extend the system model to incorporate a hybrid feature-space control scheme that provides a formalism allowing for the definition of new metrics that can consistently show real differences in reaching task performance for a nominal user.

\section{Assistive Device Development in the Literature}\label{ch:Introduction:sec:AssDevLit}

Within the literature various researchers has attempted to address numerous aspects of improving independent living for the visually impaired community.  One subset of locomotive navigational theme assistive devices constrain their framework to the recognition of text on signs within the environment (indoor or outdoor) to alert the user of conventional navigational markers used by sighted individuals position localization.

Work by Mattar~\etal~\cite{MATTAR05} and Silapachote~\etal~\cite{SILAPACHOTE05} involves detection, recognition, and identification\footnote{Text conveyed to the user through synthesized speech} of text based signs within the environment for the purpose of improved mobility.  Their system, named VIDI (Visual Integration and Dissemination of Information), acquires images from a head mounted camera unit. While sufficient for whole-body egocentric navigation, that choice of camera placement would be inefficient in the interface design for a reaching task as mapping to the reference frame of the hand would be exceedingly difficult.

%Depending on the integration of ground truth data, the localization of the user may be absolute or relative.
Sudol~\etal~\cite{SUDOL10} proposed a system named LookTel that captures video from a mobile phone camera and streamed it to a desktop base station for feature extraction and object recognition.  Object identifier tags were then sent to the mobile application which would vocalize the name of the recognized object to the user.  With the assistance of human operator intervention at the base station, the mobile user could also request assistance with tasks such as identifying their current location and/or establishing a path to a destination, either directly or via waypoints.

Chen and Yuille~\cite{CHEN04} also proposed a client-server based architecture for text recognition on signs for urban navigation, but their work assumes that the visually impaired user is responsible for first aiming the camera at the text region of the sign and can take a steady still image (minimal blur) to supply the input for the recognition application.

Another large proportion of the assistive devices for the visually impaired are designed as path planning navigational aids. Systems such as the \textit{GuideCane}~\cite{Bib:Ulrich} and the \textit{NavBelt}~\cite{Bib:Shoval}; use ultrasound, laser rangefinders, or stereoscopic camera rigs~\cite{Bib:Audette} to detect
obstacles.

Others, such as Coughlan and Manduchi~\cite{COUGHLAN09} also proposed a similar navigational aid, but it is based on identification of strategically placed colored markers.  This severely limits the usage of such a scheme in anything other than a controlled environment.  The system developed by Hile~\etal~\cite{HILE09} chose to integrate GPS-data with their image based model for the construction of a navigation path for pedestrian wayfinding.

Zawrotny~\etal~\cite{ZAWROTNY06} proposed a novel configuration for a haptic interface to feel the visual environment surrounding the user.  Their system uses light-to-tactile transducer units mounted on the dorsal surface of each finger.  The main transducer mechanism was comprised of a laser, solenoid, spring, and phototransistor.  When a properly oriented edge is found through a change in the reflectance of the modulated laser beam, the mechanism begins to vibrate through oscillatory actuation of the solenoid against the spring mount.  The tactile output would provide a constant 10Hz vibration signal as evidence of textural differences in the environment directly in line with the beam.  The system did not aid the user in locating a desired object within the immediate environment.  Arbitrary textural differences could not readily be identified as being edges of an obstacle in an open path versus opening in an oblique surface.  That task was wholly dependant on the application of the user's intelligence.

In a subsequent publication~\cite{STETTEN07} (from the same group of researchers as Zawrontny~\etal) Stetten~\etal, described an update that replaced the laser with a miniature camera and a vibrotactor.  The system, now called FingerSight\textsuperscript{\texttrademark}, also evolved in purpose.  They proposed that once an object is identified, gesturing with the finger can be used to enact remote control of the object.  They describe an example of remotely flicking a light switch at a distance.  However, no substantive details are given regarding which computer vision techniques are used for object identification, nor how finger movements would be detected and classified as command gestures.

Wanatabe~\etal~\cite{WANATABE07} present a ``WEarable walKing'' (WEK) camera based assistive system which provides dead reckoning through optical flow of edge features related to surface structures in the floor.  They also attempt to delineate between way-finding edges in the floor texture and those related to stairs or other architectural structures.  The use of this type of design is predicated on some prior knowledge of floor surface textures in the intended indoor environment, limiting its use in unfamiliar indoor locations.

Yuan and Manduchi~\cite{YUAN05} presented a virtual ``white cane'' range sensing device that employs active triangulation.  Its use is modeled upon the characteristic pivoted sweep of a physical white cane.  The authors use an Extended Kalman Filter to deal with the unknown, but approximately angular velocity of the user's sweeping motion.  One of the key usability issues with this system is the need for training.  The incremental planar scan depth and detection of obstacles is dependant on a consistent sweep pattern and rate.

A different white cane scheme, proposed by Kaneko~\etal~\cite{KANEKO03}, for indoor environment navigation involved a system reliant on the installation of ceiling mounted beacons which broadcast localization codes.  The user was notified of their current position within the map, via a receiver unit carrier on their person, as they passed beneath a beacon.  Navigation from beacon to beacon way-points was accomplished through following a coloured line path marked on the floor.  A colour sensor mounted on the ``white cane'' produced a vibrotactile output signal when the cane swept over the coloured guide line, providing an intermittent bearing signal.  This, as with many other approaches in the literature require a significant level of augmentation to existing building or environment infrastructure which severely limits their likelihood of adoption.

Bigham~\etal presented an assistive system called VizWiz in~\cite{BIGHAM10a, BIGHAM10b} that heavily relies on remote human interaction.  The system encompasses a broad scope of object and visual scene identification tasks through the data fusion of captured images and recorded verbal requests.  The mobile phone application portion of the system captured images from the built-in camera and with verbal requests from the visually impaired user transmitted the data to a remote human assistant for interpretation within the context of the accompanying image.  The requested tasks under investigation were primarily in the categories of \textit{Identification}, \textit{Description}, \textit{Spatial}, \textit{Reading}, and \textit{Answering}~\cite{BRADY11}.  Their solution (called VizWiz::LocateIt) to \textit{Spatial} task bears some similarity to our proposed method in that they direct the hand-held camera to the target object via motion cues.  Similar to our work, they advocate the use of a scale invariant feature transform (SIFT) based technique for objection recognition.  In contrast, they provide motion cues to the user audibly through their ``sonification'' application module.  The authors also do not report any detailed performance data about the usage of the system.

These types of mobile way-finding systems either incorporate the obstacles into a map of the environment so that an unobstructed path can be communicated to the user, or they communicate the location of any immediate obstacles and let the user choose how to deviate from the current path to avoid the pending obstacle~\cite{Bib:Ram, Bib:Meers}. The prevalence of the latter scheme is based on the notion that, within mobile robotics, artificial intelligence is the fundamental limiting factor~\cite{Bib:Parker}.  With current technology, an autonomous robot's ability to sense and report far surpasses it ability to process information about its environment, driving its decision-making capability.  As such, many researchers chose to leave the process of path planning to the much more versatile decision making capability of the human user.  This approach is analogous to the cooperative interaction of a visually impaired person and their seeing-eye dog companion.  In the partnership between dog and human, the dog provides navigation information and the human decides how to act upon that information.

Despite past research successes, adaptation of robotics and computer vision advancements to assistive devices lags behind the state of the art in mobile robotics research.  One of the issues in this lag in research is the difficulty with incorporating the human user into models of performance and stability of such systems.  The human, as the plant, within the control loop can make it very difficult to design a controller that provides a measurable, stable system performance.  This fundamental issue will be addressed from multiple perspectives throughout the body of this work.

In this work we present a solution to an alternate, related robotics problem that is of great significance to the visually impaired; a goal-oriented, guided reaching task.  The goal is very similar to the familiar robotics problem of servoing a robotic manipulator tool to a target object, but has been greatly under-investigated in the context of assistive devices.  There is little reported in the literature, and the overwhelming majority of those assistive devices operate under the assumption that the user is somehow able to accurately aim a still camera at the target object.  Or in the case of processing a video, that the user's visual survey (camera sweep) of the scene is systematic so that the camera alignment with the target object will automatically occur.

Drawing the obvious parallel of the human user's arm as the robotic manipulator, we approach the solution to this problem by creating a wearable assistive device that guides the user's hand to a given target object to complete the reaching task.  There is some work reported in the literature, but with only moderate applicability.
%%%

One possible haptic solution to a visual servoing model of the problem could have an architecture similar to the exoskeleton type of force-feedback devices proposed by various researchers (Bergamasco~\etal~\cite{Bib:Bergamasco}; Gupta and O'Malley~\cite{GUPTA06}; Perry and Rosen~\cite{PERRY06}.  These type of system essentially force the user's arm to follow a desired path for task completion. Consideration of this class of assistive device led us to pursue a different approach that we feel would be more efficient and considerably less cumbersome for the user.  Assuming that the user's only impairment is with their sight, implementing a mechatronic system to physically drive their arm through a sequence of motions that they are amply capable on their is too intrusive. An exoskeletal apparatus approach could be highly valuable if the human user has some degree of motor control infirmity, but for user's without a motor impairment, the unnecessary weight and size of the required components restricts their freedom to interact with the environment instead of enhancing it.

Even though a guided reaching task is quite a different problem than the traditional locomotive navigation problem mentioned above, we can still use the motivating example of a seeing-eye guide dog to delineate the interaction between the user and system.  Applying the analogy, we assert that the human retains supervisory control of the dog-human team, while the dog is only responsible for sensing the environment and passing that navigation information along to the human.  In the same fashion, the proposed assistive device system identifies the valid target object and provides generalized guidance cues necessary to complete the guided reaching task.  The user has the freedom to follow or ignore those cues at will.  However, assuming the human chooses to follow those guidance cues provided by the dog, they are free do so with any arbitrary gait. In the context of a reaching task, the analogy translates to little or no limitation imposed by the assistive device on the pose and precise path that the user's arm follows during the act of the completing the reaching task.

The seeing-eye guide dog analogy also serves to differentiate our approach from another field of inquiry into assistive devices for the blind known as sensory substitution.  The goal of most sensory substitution devices, (the classic example is the Optacon~\cite{Bib:Linvill}) is to generate tactile cues to represent a scene pictorially.  Patterns of raised and lowered pins attempt to give the user a tactile sense of the sampled scene.  In essence, a haptic coding scheme for the image data, representing a visual scene in a manner the user can perceive via tactile input.  Our approach, by contrast, can be considered the transmission of semantic information about a scene. Instead of attempting to represent a scene in a tactile format, we focus on conveying motion cues to the wearer.  Some prior work by Tan and Pentland~\cite{TAN97, TAN05} exists in the field of sensory substitution that exploits the phenomenon of sensory saltation to give general direction cues to a user. Sensory saltation is achieved through vibratory stimulation of various cutaneous sites in sequence, the observer perceives the motion of the stimulus at interpolated points between the stimulus sights. While an interesting perceptual phenomenon, that technique may not be entirely suitable for the target tracking motor control cuing required for a guided reaching task.

\section{Unique Application Considerations}\label{ch:Introduction:sec:UniqueApp}

There are three fundamental technical challenges in the system.  The first problem is to design a visual servoing system that will
recognize a desired object and generate motion cues towards it.  By itself, this is a familiar problem from robotics research~\cite{Bib:Corke, Bib:Hutchinson}, but with the added complication that the ``robot arm'' is now, in fact, a human arm.  Since the system is human centric, it can not be easily proven to be a controllable closed-loop system.  Within the loop, there can be significant issues with user's perception of control signals and their free will to choose whether or not to follow commands generated by the controller.  Even under the assumption that the user makes a best effort to obey the control issued, their ability to do so can be affected by intrinsic physiological and psychological factors such as fatigue, frustration, or confusion.  These are certainly not issues inherent in conventional robotic systems.  However, for the purposes of this work we consider it a given that the user's intent is to follow the control to the best of their ability and thus refer to it as quasi closed-loop control system.

The second fundamental challenge is the development of a technique to communicate the necessary motion cues to the human user.  Since the primary human sensory input channel (sight) is unavailable, we are forced to choose between lower bandwidth channels:  namely audible input or haptic input.  Using audible cues as a primary input channel is less desirable since the visually impaired already rely extensively on their hearing.  Such additional audible input could be considered more akin to interference on the channel rather than a desired signal.
%\textbf{Note: I remember finding a paper that supported the previous thought.  I will have to dig it up to fill in the reference.}
Accordingly, the use of tactile cues to direct the motion of the user's arm was selected as the primary input channel to the user. The technique of using small forces to influence a user's direction has already been proven by other researchers in the field of assistive devices (especially in the case of the~\textit{GuideCane}).  While this tends to be used in assistive devices for navigation during a locomotion task, the evidence shows that users, visually impaired or otherwise, are very responsive to tactile cuing schemes.

The third challenge is tightly intertwined with the previous one.  It is the design and/or application of relevant performance measures for this unique class of human motor-control application.  As we discuss in further detail in Chapter~\ref{ch:HumanMotorPerf}: Human Motor Performance, various metrics for human motor performance can be found within the literature, but none of them fully address the distinct difference inherent in performing this type of task without normal visual input to the visio- and neuromotor systems of a human.  Previous studies of human motor performance related to reaching tasks have been conducted with sighted individuals.  Those studies initiated by Paul Fitts, brought about the development of Fitts' Law~\cite{FITTS54}.  Through Fitts, and numerous other researchers, Fitts' Law has been used extensively in the evaluation of goal-oriented reaching tasks~\cite{FERRIER98, MURATA99, MURATA99a, YANG-JIN02, GROSSMAN04}, but until recently its use has been with subjects that are sighted individuals.  The lack of the normal human vision within the neuromotor control feedback loop significantly alters a user's motion planning capability.  Without a usable set of performance measures, as with any control system, it is difficult to evaluate the behaviour of the system and then quantify the performance difference under alternate conditions.

\section{Research Goal}\label{ch:Introduction:sec:ResearchGoal}
The purpose of this research is to investigate the nature of guiding a visually impaired person's hand towards an object they wish to grasp through the use of an assistive device.  From a basis of the natural process by which a person approaches a sighted grasping task, we have designed an assistive device system called \texttt{aiReach} (assistive image-based Reaching) to aid a visually impaired person in performing the initial stage the process, reaching for the object.  To do so, a prototype assistive device was constructed as an experimental platform.  An illustration of the hardware components that make up the prototype is given in Figure.~\ref{fig:proto:aiReach}\subref{subfig:proto:system_block_diagram}, and a picture of the wearable components of the prototype is given in Figure.~\ref{fig:proto:aiReach}\subref{subfig:proto:prototype_glove_pic}.
Two versions of prototype were built during the course of this work.  The first was an initial proof-of-concept to investigate general unknown usability issues.  The second involved upgrades and revisions to the microcontroller and vision system software to enable proportional control and increased frame rate, respectively.
\begin{figure}[ht]
\centering
    \subfloat[][]{\label{subfig:proto:system_block_diagram}
    \includegraphics[width=0.45\textwidth]{./ProposalDiagrams/BlockDiagram.pdf}}
    %
    \subfloat[][]{\label{subfig:proto:prototype_glove_pic}
    \includegraphics[width=0.45\textwidth]{cam_glove_duane.jpg}}
        \caption[\texttt{aiReach} prototype assistive device]{The \texttt{aiReach} (pronounced `eye-Reach') system initial prototype: \subref{subfig:proto:system_block_diagram} illustration representing the three hardware components and their interconnection in the experimental prototype; \subref{subfig:proto:prototype_glove_pic} picture of the wearable portion of the system on the author's arm.}
    \label{fig:proto:aiReach}
\end{figure}
In our prototype the vision system consists of a small, lightweight, colour CCD-camera and a PC workstation.  The glove mounted camera is connected to the PC via a Universal Serial Bus (USB) cable. The glove is also equipped with four vibrating disc motors; one each on the palm, back of the hand, and either side. The placement of the four motors
corresponds to the intended direction of motion of the user's hand
that generates corresponding horizontal, vertical, or depth movements in the image plane.
The microcontroller is connected to the PC via a RS232 serial cable and receives the trajectory data to general the appropriate motion cues.

The investigation includes analysis and a review of various computer vision techniques for object detection and tracking; using a Discrete Event System (DES) hybrid control approach to model the system; the development of a framework for measuring the performance of a goal-oriented reaching task for a non-sighted user; and a proposed motor performance model for this type of reaching task with the DES hybrid model.

\subsection{Scope of the Work}\label{ch:Introduction:sec:ReserachGoal:ssec:Scope}

Neither the construction of a commercially viable device, nor a prototype capable of normal operation outside of the laboratory environment is within the scope of this work.  In either case the significant limiting factor is cost of system components.  In particular, the cost of providing hardware acceleration to minimize the computation time of image processing required for the object detection and tracking, and the power source (high capacity battery) for portability.

The type of limb movement involved is a constrained reaching task, as opposed to a full grasping task.  The delineation being, that a reaching task would terminate at a point where the hand is sufficiently close to the target that grasping could occur with minimal probing of the immediate region of the task space.  We do not address the problem of providing any pose control or force control law that can guide the user's hand to completion of the subsequent grasping task that would follow.  This is considered a separate problem that is already naturally solvable through the user's precise motor control and sensory capability to probe the local area with their fingers to determine the appropriate pose and force necessary to grasp the object safely and securely.

Within the scope we present the design, modeling, and performance analysis of a prototype system used in experiments with voluntary subjects performing guided reaching tasks under an unsighted condition.  The analysis is done on recorded trajectories from the numerous trials of guided reaching tasks performed during the experiments.  The participants in each study are allowed and encouraged to respond to the motion cues in a manner natural and comfortable to them, so that the system response is indicative of realistic movement behaviours.

\subsection{Research Contributions}\label{ch:Introduction:sec:ResearchGoal:ssec:Contributions}
Summarized below is a list of contributions we propose this work will make to the assistive device and HCI research communities.
\begin{itemize}
\item a proof of concept wearable assistive device to aid visually impaired users in a guided reaching task
\item a feature-space hybrid control model for the system (non-sighted user - assistive device) behaviour during a reaching task using a somatosenory interface.
%\item verification that Fitts' Law does not readily apply for guided reaching tasks for non-sighted subjects.
\item demonstration using Fitts' Law that non-sighted reaching is not a simple ballistic pointing task.
\item a proposed set of metrics to measure the guided reaching task performance that can quantify the effect of altering model parameters.
\item a novel method for distance estimation with calibrated monocular vision using a non-uniform weighting scheme for relating object size to point-feature scale.
\end{itemize}

\section{Organization of the Work}\label{ch:Introdution:sec:Organization}

The remainder of this work is structured as follows: Chapter~\ref{ch:SysDesc} gives an overview of the breakdown of the system. In that chapter we also provide the reader with some important insight in the unique considerations that make this problem very complex and how they motivated particular design considerations.  Chapter 3 discusses the visual feature-space control law we have developed and the Discrete Event System (DES) model employed.  Within that chapter, we address issues of visual feature extraction techniques and the need for feature-space based hybrid systems controller because of the absence of a known task-space and ground truth.  The chapter outlines some trade offs between various feature tracking scheme investigated, and proposes a simple but novel technique for distance estimation employing a scale covariant and illumination invariant feature tracker.

Chapter 4 reviews the existing literature on human motor performance relating to reaching tasks and discusses the applicability of Fitts' Law, from the field of psycho-motor movement modeling, to measure the degree of success for goal-oriented reaching tasks.  We also provide the development of new performance metrics based on the hybrid systems feature-space control model presented in the preceding chapter.

Chapter 5 describes the various experiments conducted to justify the material presented in this work.
%\textbf{Note: Ken, should I provide a couple sentences briefly describing the breakdown of experiments?}.
Conclusions and recommendations for Future Work are provided in Chapters 6 and 7, respectively.  